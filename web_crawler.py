# ç½‘é¡µçˆ¬å–
import requests # å¯¼å…¥requestsåº“ï¼šç”¨äºå‘é€ HTTP è¯·æ±‚è·å–ç½‘é¡µå†…å®¹
from bs4 import BeautifulSoup # å¯¼å…¥BeautifulSoupåº“ï¼šç”¨äºè§£æ HTML é¡µé¢ï¼Œæå–æ•°æ®
import pandas # æŠ“å–çš„ç½‘é¡µæ•°æ®åˆ©ç”¨pandasåº“è½¬æ¢ä¸ºcsvæ–‡ä»¶å­˜å‚¨
import time
import sqlite3
from readability import Document # readability æ˜¯ä¸€ä¸ªç½‘é¡µæ­£æ–‡æå–å·¥å…·ï¼Œå¯ä»¥ä» HTML é¡µé¢ä¸­æå–å‡ºä¸»è¦å†…å®¹
# import jieba
from concurrent.futures import ThreadPoolExecutor
from keyword_extractor import extract_keywords  # å¼•å…¥å…³é”®è¯æå–æ¨¡å—

# è¿æ¥ SQLite æ•°æ®åº“
conn = sqlite3.connect("articles.db")
cursor = conn.cursor() # åˆ›å»ºä¸€ä¸ªæ¸¸æ ‡å¯¹è±¡ï¼Œæ‰§è¡ŒSQLè¯­å¥ï¼ˆæŸ¥è¯¢ã€æ’å…¥ã€æ›´æ–°ç­‰ï¼‰
cursor.execute('''CREATE TABLE IF NOT EXISTS pages 
                  (id INTEGER PRIMARY KEY, title TEXT, url TEXT, content TEXT, keywords TEXT)''')# åˆ›å»ºä¸€ä¸ªåä¸ºpagesçš„è¡¨
conn.commit() # æäº¤ SQL è¯­å¥çš„æ›´æ”¹ï¼Œè®©æ•°æ®åº“ä¿å­˜åˆ›å»ºçš„ pages è¡¨

#å­—æ®µè§£é‡Š
#å­—æ®µå	æ•°æ®ç±»å‹	è¯´æ˜
#id	INTEGER PRIMARY KEY	ä¸»é”®ï¼Œè‡ªåŠ¨é€’å¢ï¼Œå”¯ä¸€æ ‡è¯†æ¯ä¸ªç½‘é¡µ
#title	TEXT	ç½‘é¡µæ ‡é¢˜
#url	TEXT	ç½‘é¡µé“¾æ¥
#content	TEXT	ç½‘é¡µæ­£æ–‡å†…å®¹ï¼ˆçˆ¬å–çš„æ–‡æœ¬ï¼‰
#keywords	TEXT	å…³é”®è¯ï¼ˆç”¨äºæœç´¢ä¼˜åŒ–ï¼‰

# è®¾ç½®ç›®æ ‡ç½‘å€
# è®¿é—®è¿‡çš„ URLï¼Œé¿å…é‡å¤çˆ¬å–
visited_urls = set()
# url = 'https://www.baidu.com/'

# æŠ“å–ç½‘é¡µå‡½æ•°
def fetch_page(url):
    headers = {
        'User-Agent': 'studyCrawler (+https://github.com/l1xiaobei/Python_SearchEngine)'
    }
    # response = requests.get(url, headers=headers)
    # response = requests.get(url)
    # if response.status_code == 200: # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸ
    #     response.encoding = 'utf-8'  # å¤„ç†ä¸­æ–‡ä¹±ç 
    #     return response.text # è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒåŒ…å«äº†ä»è¯¥ url è·å–åˆ°çš„ HTML é¡µé¢ã€‚
    # else:
    #     print("[æç¤ºä¿¡æ¯].æŠ“å–ç½‘é¡µæ—¶å‡ºç°é”™è¯¯ï¼Œé”™è¯¯ç ä¸º:" + str(response.status_code) + "!")
    #     return None
    # ä½¿ç”¨tryè¯­å¥è¿›è¡Œå¼‚å¸¸å¤„ç†
    for i in range(3):  # æœ€å¤šé‡è¯• 3 æ¬¡
        try:
            # response = requests.get(url)        
            response = requests.get(url, headers=headers, timeout=10)  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé¿å…å¡ä½ï¼Œå•ä½ä¸ºç§’
            response.raise_for_status()  # è¯¥æ–¹æ³•ç”¨äºæ£€æŸ¥ HTTP å“åº”çŠ¶æ€ç ï¼Œå¦‚æœ HTTP çŠ¶æ€ç ä¸æ˜¯ 200ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸requests.exceptions.HTTPError
            response.encoding = 'utf-8'
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"[æç¤ºä¿¡æ¯].æŠ“å–ç½‘é¡µæ—¶å‡ºç°é”™è¯¯,ç¬¬ {i+1} æ¬¡è¯·æ±‚å¤±è´¥: {e}")
            time.sleep(2)  # ä¼‘æ¯ 2 ç§’åé‡è¯•
    
# ä½¿ç”¨ BeautifulSoup è§£æ HTML é¡µé¢
def parse_page(url):
    if url in visited_urls:
        return None
    visited_urls.add(url)  # è®°å½•å·²çˆ¬å– URL
    html = fetch_page(url) 
    if not html:
        return None
    soup = BeautifulSoup(html, 'lxml') # ä½¿ç”¨lxmlè§£æå™¨è§£æçˆ¬å–åˆ°çš„htmlç½‘é¡µå†…å®¹
        # æå–æ ‡é¢˜
        # title = soup.find('title')
        # if soup.find('title'):
        #     title = soup.find('title').get_text()
        # else:
        #     print('[æç¤ºä¿¡æ¯].æŠ“å–ä¸åˆ°æ ‡é¢˜!') # åŠ ä¸€ä¸ªéªŒè¯ç¡®ä¿æ ‡é¢˜å­˜åœ¨
    title = soup.find('title').get_text() if soup.find('title') else (print("[æç¤ºä¿¡æ¯].æŠ“å–ä¸åˆ°æ ‡é¢˜!") or '')
        # æå–æ­£æ–‡
        # body = soup.find('body')
        # if soup.find('body'):
        #     body = soup.find('body').get_text()
        # else:
        #     print('[æç¤ºä¿¡æ¯].æŠ“å–ä¸åˆ°æ­£æ–‡!') # åŠ ä¸€ä¸ªéªŒè¯ç¡®ä¿æ­£æ–‡å­˜åœ¨
    # body = soup.find('body').get_text() if soup.find('body') else (print("[æç¤ºä¿¡æ¯].æŠ“å–ä¸åˆ°æ­£æ–‡!") or '')
    # æå–æ­£æ–‡ ä½¿ç”¨readabilityåº“
    doc = Document(html) # è§£æ HTMLï¼Œè‡ªåŠ¨è¯†åˆ«æ­£æ–‡ 
    body = doc.summary() if doc.summary() else (print("[æç¤ºä¿¡æ¯].æŠ“å–ä¸åˆ°æ­£æ–‡!") or '')# è·å–ä¸»è¦å†…å®¹ï¼ˆä»¥ HTML æ ¼å¼è¾“å‡ºï¼‰
        # æå–æ—¥æœŸ
        # date = soup.find('span', {'class': 'publish-date'}).get_text() # æå–ç½‘é¡µæ—¥æœŸ
        # author = soup.find('span', {'class': 'author-name'}).get_text() # æå–ä½œè€…ä¿¡æ¯
        ##ä¸´æ—¶
            # è¾“å‡ºæ ‡é¢˜å’Œéƒ¨åˆ†æ­£æ–‡å†…å®¹ï¼ˆä¸ºäº†é¿å…æ‰“å°è¿‡å¤šæ–‡æœ¬ï¼Œè¿™é‡Œåªè¾“å‡ºå‰500ä¸ªå­—ç¬¦ï¼‰
    # æå–å…³é”®è¯
    # keywords = ",".join(jieba.analyse.extract_tags(body, topK=5))
    # ä½¿ç”¨ extract_keywords è‡ªåŠ¨æå–å…³é”®è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    keywords = extract_keywords(body, topK=5)

    print(f"[æç¤ºä¿¡æ¯].æˆåŠŸæŠ“å–: {url}")
    print(f"[æç¤ºä¿¡æ¯].æ ‡é¢˜: {title}\nå…³é”®è¯: {keywords}\n")

    return title, url, body, keywords

# å­˜å‚¨æ•°æ®åˆ° SQLite
def save_to_db(title, url, body, keywords):
    cursor.execute("INSERT INTO pages (title, url, content, keywords) VALUES (?, ?, ?, ?)",
                   (title, url, body, keywords))
    conn.commit()

# def save_page(url):
#     result = parse_page(url)
#     if result:
#         title, body = result 
#         # å°†æŠ“å–çš„æ•°æ®å­˜å‚¨ä¸º CSV æ–‡ä»¶
#         # åˆ›å»ºå­—å…¸data
#         data = {'title': [title], 'link': [url], 'body': [body]}
#         # è¿™é‡Œçš„ä¸­æ‹¬å· [ ] ç”¨äºåˆ›å»ºåŒ…å«å•ä¸ªå…ƒç´ çš„åˆ—è¡¨
#         # åœ¨ Pandas ä¸­ï¼ŒDataFrame æ˜¯ä¸€ä¸ªäºŒç»´çš„ã€è¡¨æ ¼å‹çš„æ•°æ®ç»“æ„ï¼Œéœ€è¦è¡Œå’Œåˆ—æ¥ç»„ç»‡æ•°æ®ã€‚
#         # å½“åˆ›å»ºä¸€ä¸ª DataFrame æ—¶ï¼Œå¦‚æœæä¾›çš„æ•°æ®æ˜¯å•ä¸ªå€¼è€Œä¸æ˜¯åˆ—è¡¨æˆ–æ•°ç»„ï¼Œ
#         # Pandas ä¼šå°†å…¶è§†ä¸ºæ ‡é‡å€¼ï¼Œå¹¶æŠ›å‡ºä¸€ä¸ªé”™è¯¯ï¼Œå› ä¸ºå®ƒä¸çŸ¥é“å¦‚ä½•å°†è¿™äº›æ ‡é‡å€¼ç»„ç»‡æˆè¡Œå’Œåˆ—ã€‚
#         # é€šè¿‡å°†æ¯ä¸ªå€¼åŒ…è£…åœ¨åˆ—è¡¨ä¸­ï¼Œå¯ä»¥æ˜ç¡®åœ°å‘Šè¯‰ Pandas æ¯ä¸ªå€¼åº”è¯¥è¢«è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„å…ƒç´ ï¼Œ
#         # è€Œä¸æ˜¯æ•´ä¸ª DataFrame çš„åˆ—ã€‚
#         # è¿™æ ·ï¼ŒPandas å°±å¯ä»¥æ­£ç¡®åœ°å°†è¿™äº›æ•°æ®è½¬æ¢ä¸º DataFrameï¼Œæ¯ä¸ªåˆ—è¡¨ä¸­çš„å…ƒç´ å¯¹åº” DataFrame çš„ä¸€è¡Œã€‚
#         df = pandas.DataFrame(data) # dfå³dataframeï¼Œæ˜¯ä¸€ä¸ªè¡¨æ ¼ç±»å‹çš„æ•°æ®ç»“æ„
#         df.to_csv('articles.csv', index=True) # å°†dfä¿å­˜ä¸ºåä¸ºarticlesçš„csvæ–‡ä»¶
#     else:
#         print("[æç¤ºä¿¡æ¯].æ— æ³•è§£æé¡µé¢!")

# çˆ¬è™«è°ƒåº¦
def start_crawler(start_urls, max_pages=10):
    to_crawl = set(start_urls)
    crawled = set()
    
    with ThreadPoolExecutor(max_workers=5) as executor:  # çº¿ç¨‹çˆ¬å–
        while to_crawl and len(crawled) < max_pages:
            url = to_crawl.pop()
            new_links = executor.submit(parse_page, url).result()
            if new_links:
                save_to_db(*new_links)
            crawled.add(url)

    print(f"\nğŸ¯ çˆ¬å–å®Œæˆï¼Œå·²çˆ¬å– {len(crawled)} ä¸ªç½‘é¡µ")

# 5ï¸âƒ£ å¯åŠ¨çˆ¬è™«
start_urls = ["https://www.baidu.com"]
start_crawler(start_urls, max_pages=20)

# 6ï¸âƒ£ å…³é—­æ•°æ®åº“
conn.close()